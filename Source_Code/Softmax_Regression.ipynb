{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[0.14409682 0.05301026 0.01950138 0.39169577 0.39169577]\n"
     ]
    }
   ],
   "source": [
    "# softmax regression\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "z = np.array([5,4,3,6,6])\n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    # z--> linear part.\n",
    "    \n",
    "    # subtracting the max of z for numerical stability.\n",
    "    exp = np.exp(z - np.max(z))\n",
    "    \n",
    "    # Calculating softmax for all examples.\n",
    "    for i in range(len(z)):\n",
    "        exp[i] /= np.sum(exp[i])\n",
    "        \n",
    "    return exp\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A\n",
    "\n",
    "def number_classes(y):\n",
    "    c = 0\n",
    "    lst_class = []\n",
    "    dict = {}\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in lst_class:\n",
    "            c += 1\n",
    "            lst_class.append(y[i])\n",
    "    return lst_class, c\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # w --> weights.\n",
    "    # b --> bias.\n",
    "    \n",
    "    # Predicting\n",
    "    z = X@w + b\n",
    "    y_hot = softmax(z)\n",
    "    \n",
    "    # Returning the class with highest probability.\n",
    "    return np.argmax(y_hot, axis=1)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return str(np.sum(y==y_hat)/len(y)*100) +\"%\"\n",
    "\n",
    "print(softmax(z))\n",
    "print(softmax_stable(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top(x, w, b, n):\n",
    "    z = x@w + b\n",
    "    y_hot = softmax_stable(z)\n",
    "    \n",
    "    #sort the y_hot\n",
    "    sort = np.sort(y_hot)\n",
    "    sort_convert = sort[::-1]\n",
    "    \n",
    "    top_n = sort_convert[:n]\n",
    "    top_n_index = []\n",
    "    for j in range(len(top_n)):\n",
    "        for i in range(len(y_hot)):\n",
    "            if y_hot[i] == top_n[j]:\n",
    "                top_n_index.append(i)\n",
    "    \n",
    "    top_list = {}\n",
    "    for i in range(n):\n",
    "        top_list[(dict_convert[classes[top_n_index[i]]])] = str(sort_convert[i] * 100) + \"%\"\n",
    "    return top_n, top_n_index, top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"dataset.csv\")\n",
    "df_raw = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_raw.iloc[:,:-1].to_numpy()\n",
    "y = df_raw.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Fungal infection', 'Fungal infection', 'Fungal infection', ...,\n",
       "       'Covid', 'Covid', 'Covid'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of classes\n",
    "\n",
    "dict = {}\n",
    "count = 0\n",
    "for x in y:\n",
    "    if x not in dict:\n",
    "        dict[x] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of convert classes\n",
    "dict_convert = {}\n",
    "for x in dict:\n",
    "    dict_convert[dict[x]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change text to integer\n",
    "for i in range(len(y)):\n",
    "    y[i] = dict[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, c = number_classes(y)\n",
    "y_hot = one_hot(y, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -np.mean(np.log(y_hot[np.arange(len(y)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lr, c, epochs):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # lr --> Learning rate.\n",
    "    # c --> Number of classes.\n",
    "    # epochs --> Number of iterations.\n",
    "    \n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias randomly.\n",
    "    w = np.random.random((n, c))\n",
    "    b = np.random.random(c)\n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Calculating hypothesis/prediction.\n",
    "        z = X@w + b\n",
    "        y_hat = softmax(z)\n",
    "        \n",
    "        # One-hot encoding y.\n",
    "        y_hot = one_hot(y, c)\n",
    "        \n",
    "        # Calculating the gradient of loss w.r.t w and b.\n",
    "        w_grad = (1/m)*np.dot(X.T, (y_hat - y_hot)) \n",
    "        b_grad = (1/m)*np.sum(y_hat - y_hot)\n",
    "        \n",
    "        # Updating the parameters.\n",
    "        w = w - lr*w_grad\n",
    "        b = b - lr*b_grad\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n",
    "        losses.append(loss)\n",
    "        # Printing out the loss at every 100th iteration.\n",
    "        if epoch%100==0:\n",
    "            print('Epoch {epoch}==> Loss = {loss}'\n",
    "                  .format(epoch=epoch, loss=loss))\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vs sample data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0==> Loss = 15.945728091562515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-aeef52f5c68f>:39: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100==> Loss = 0.002941958754275336\n",
      "Epoch 200==> Loss = 0.00239918080103578\n",
      "Epoch 300==> Loss = 0.0021471607672331012\n",
      "Epoch 400==> Loss = 0.001990989457306159\n",
      "Epoch 500==> Loss = 0.00188541627465411\n",
      "Epoch 600==> Loss = 0.0018079147530272305\n",
      "Epoch 700==> Loss = 0.0017471384188264992\n",
      "Epoch 800==> Loss = 0.0016975651940782299\n",
      "Epoch 900==> Loss = 0.0016561881019201453\n",
      "Epoch 1000==> Loss = 0.0016211138198102646\n",
      "Epoch 1100==> Loss = 0.0015910292502261847\n",
      "Epoch 1200==> Loss = 0.0015649721251688106\n",
      "Epoch 1300==> Loss = 0.0015422133355567297\n",
      "Epoch 1400==> Loss = 0.001522187789514879\n",
      "Epoch 1500==> Loss = 0.0015044500493187564\n",
      "Epoch 1600==> Loss = 0.001488644207451563\n",
      "Epoch 1700==> Loss = 0.0014744826314878254\n",
      "Epoch 1800==> Loss = 0.0014617305481069286\n",
      "Epoch 1900==> Loss = 0.001450194626552295\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "w, b, l = fit(X_train, y_train, lr=1, c=len(classes), epochs=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set accuracy:  99.96768982229402%\n",
      "test set accuracy:  99.9515503875969%\n"
     ]
    }
   ],
   "source": [
    "# Accuracy for training set.\n",
    "train_preds = predict(X_train, w, b)\n",
    "print(\"train set accuracy: \", accuracy(y_train, train_preds))\n",
    "\n",
    "# Accuracy for test set.\n",
    "# Flattening and normalizing.\n",
    "test_preds = predict(X_test, w, b)\n",
    "print(\"test set accuracy: \", accuracy(y_test, test_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000000e+00, 2.43891460e-30, 5.55912089e-32, 3.68280355e-32,\n",
       "        3.08691828e-32]),\n",
       " [41, 37, 35, 38, 10],\n",
       " {'Covid': '100.0%',\n",
       "  'Acne': '2.4389145997569186e-28%',\n",
       "  'Arthritis': '5.5591208901950144e-30%',\n",
       "  'Urinary tract infection': '3.682803548309472e-30%',\n",
       "  'Hypertension ': '3.086918275335293e-30%'})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[5115]\n",
    "predict_top(x, w, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_x():\n",
    "    n = random.randint(10,30)\n",
    "    random_index = random.sample(range(1, len(X[5] - 1)), n)\n",
    "    zero_matrix = np.zeros(len(X[5]))\n",
    "    for i in random_index:\n",
    "        zero_matrix[i] = 1\n",
    "    return n, zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.88468886, 0.05060782, 0.01627917, 0.01589509, 0.01255453]),\n",
       " [22, 24, 19, 12, 23],\n",
       " {'Hepatitis D': '88.46888552221093%',\n",
       "  'Alcoholic hepatitis': '5.060782175048032%',\n",
       "  'hepatitis A': '1.627917025887722%',\n",
       "  'Cervical spondylosis': '1.589509089854039%',\n",
       "  'Hepatitis E': '1.2554525256490545%'})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, k = create_random_x()\n",
    "n, k\n",
    "predict_top(k, w, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\anacoda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#train model with sklearn\n",
    "import numpy as np \n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train\n",
    "logreg = linear_model.LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = logreg.predict(X_test)\n",
    "print (\"Accuracy: %.2f %%\" %(100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
