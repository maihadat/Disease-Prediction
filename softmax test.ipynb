{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[0.14409682 0.05301026 0.01950138 0.39169577 0.39169577]\n"
     ]
    }
   ],
   "source": [
    "# softmax regression\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "z = np.array([5,4,3,6,6])\n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    # z--> linear part.\n",
    "    \n",
    "    # subtracting the max of z for numerical stability.\n",
    "    exp = np.exp(z - np.max(z))\n",
    "    \n",
    "    # Calculating softmax for all examples.\n",
    "    for i in range(len(z)):\n",
    "        exp[i] /= np.sum(exp[i])\n",
    "        \n",
    "    return exp\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A\n",
    "\n",
    "def number_classes(y):\n",
    "    c = 0\n",
    "    lst_class = []\n",
    "    dict = {}\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in lst_class:\n",
    "            c += 1\n",
    "            lst_class.append(y[i])\n",
    "    return lst_class, c\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # w --> weights.\n",
    "    # b --> bias.\n",
    "    \n",
    "    # Predicting\n",
    "    z = X@w + b\n",
    "    y_hat = softmax(z)\n",
    "    \n",
    "    # Returning the class with highest probability.\n",
    "    return np.argmax(y_hat, axis=1)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return str(np.sum(y==y_hat)/len(y)*100) +\"%\"\n",
    "\n",
    "print(softmax(z))\n",
    "print(softmax_stable(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top(x, w, b, n):\n",
    "    z = x@w + b\n",
    "    y_hot = softmax_stable(z)\n",
    "    \n",
    "    #sort the y_hot\n",
    "    sort = np.sort(y_hot)\n",
    "    sort_convert = sort[::-1]\n",
    "    \n",
    "    top_n = sort_convert[:n]\n",
    "    top_n_index = []\n",
    "    for j in range(len(top_n)):\n",
    "        for i in range(len(y_hot)):\n",
    "            if y_hot[i] == top_n[j]:\n",
    "                top_n_index.append(i)\n",
    "    \n",
    "    top_list = {}\n",
    "    for i in range(n):\n",
    "        top_list[(dict_convert[classes[top_n_index[i]]])] = str(sort_convert[i] * 100) + \"%\"\n",
    "    return top_n, top_n_index, top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"out.csv\")\n",
    "df_raw = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_raw.iloc[:,:-1].to_numpy()\n",
    "y = df_raw.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of classes\n",
    "\n",
    "dict = {}\n",
    "count = 0\n",
    "for x in y:\n",
    "    if x not in dict:\n",
    "        dict[x] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of convert classes\n",
    "dict_convert = {}\n",
    "for x in dict:\n",
    "    dict_convert[dict[x]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change text to integer\n",
    "for i in range(len(y)):\n",
    "    y[i] = dict[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, c = number_classes(y)\n",
    "y_hot = one_hot(y, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -np.mean(np.log(y_hot[np.arange(len(y)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lr, c, epochs):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # lr --> Learning rate.\n",
    "    # c --> Number of classes.\n",
    "    # epochs --> Number of iterations.\n",
    "    \n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias randomly.\n",
    "    w = np.random.random((n, c))\n",
    "    b = np.random.random(c)\n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Calculating hypothesis/prediction.\n",
    "        z = X@w + b\n",
    "        y_hat = softmax(z)\n",
    "        \n",
    "        # One-hot encoding y.\n",
    "        y_hot = one_hot(y, c)\n",
    "        \n",
    "        # Calculating the gradient of loss w.r.t w and b.\n",
    "        w_grad = (1/m)*np.dot(X.T, (y_hat - y_hot)) \n",
    "        b_grad = (1/m)*np.sum(y_hat - y_hot)\n",
    "        \n",
    "        # Updating the parameters.\n",
    "        w = w - lr*w_grad\n",
    "        b = b - lr*b_grad\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n",
    "        losses.append(loss)\n",
    "        # Printing out the loss at every 100th iteration.\n",
    "        if epoch%100==0:\n",
    "            print('Epoch {epoch}==> Loss = {loss}'\n",
    "                  .format(epoch=epoch, loss=loss))\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vs sample data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0==> Loss = 19.588297100373808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-210-aeef52f5c68f>:39: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100==> Loss = 0.01063797805145097\n",
      "Epoch 200==> Loss = 0.007767469665437666\n",
      "Epoch 300==> Loss = 0.006518366738953934\n",
      "Epoch 400==> Loss = 0.005735388506923986\n",
      "Epoch 500==> Loss = 0.005179770887277138\n",
      "Epoch 600==> Loss = 0.004757636979415175\n",
      "Epoch 700==> Loss = 0.00442189316686973\n",
      "Epoch 800==> Loss = 0.004145919340566325\n",
      "Epoch 900==> Loss = 0.003913511131367227\n",
      "Epoch 1000==> Loss = 0.003714161216757258\n",
      "Epoch 1100==> Loss = 0.003540696810812607\n",
      "Epoch 1200==> Loss = 0.00338802071911251\n",
      "Epoch 1300==> Loss = 0.003252391365811723\n",
      "Epoch 1400==> Loss = 0.0031309868189938247\n",
      "Epoch 1500==> Loss = 0.003021628732626871\n",
      "Epoch 1600==> Loss = 0.0029226009246481207\n",
      "Epoch 1700==> Loss = 0.0028325263541913452\n",
      "Epoch 1800==> Loss = 0.0027502815730238305\n",
      "Epoch 1900==> Loss = 0.0026749360748922124\n",
      "Epoch 2000==> Loss = 0.002605708639993917\n",
      "Epoch 2100==> Loss = 0.002541935495253689\n",
      "Epoch 2200==> Loss = 0.0024830467876951123\n",
      "Epoch 2300==> Loss = 0.0024285489570947207\n",
      "Epoch 2400==> Loss = 0.002378011328490217\n",
      "Epoch 2500==> Loss = 0.0023310557503201093\n",
      "Epoch 2600==> Loss = 0.002287348454211866\n",
      "Epoch 2700==> Loss = 0.002246593555728424\n",
      "Epoch 2800==> Loss = 0.002208527784456291\n",
      "Epoch 2900==> Loss = 0.002172916149417446\n",
      "Epoch 3000==> Loss = 0.002139548327755547\n",
      "Epoch 3100==> Loss = 0.00210823562198916\n",
      "Epoch 3200==> Loss = 0.002078808371448923\n",
      "Epoch 3300==> Loss = 0.002051113732071655\n",
      "Epoch 3400==> Loss = 0.002025013759119304\n",
      "Epoch 3500==> Loss = 0.002000383742114957\n",
      "Epoch 3600==> Loss = 0.0019771107520438667\n",
      "Epoch 3700==> Loss = 0.0019550923688432063\n",
      "Epoch 3800==> Loss = 0.0019342355631987887\n",
      "Epoch 3900==> Loss = 0.0019144557112593128\n",
      "Epoch 4000==> Loss = 0.001895675724442607\n",
      "Epoch 4100==> Loss = 0.0018778252793323542\n",
      "Epoch 4200==> Loss = 0.0018608401349264698\n",
      "Epoch 4300==> Loss = 0.0018446615263457984\n",
      "Epoch 4400==> Loss = 0.0018292356256341665\n",
      "Epoch 4500==> Loss = 0.001814513061552099\n",
      "Epoch 4600==> Loss = 0.001800448491339206\n",
      "Epoch 4700==> Loss = 0.0017870002183290093\n",
      "Epoch 4800==> Loss = 0.0017741298500793432\n",
      "Epoch 4900==> Loss = 0.001761801992349806\n",
      "Epoch 5000==> Loss = 0.0017499839748358133\n",
      "Epoch 5100==> Loss = 0.0017386456050685554\n",
      "Epoch 5200==> Loss = 0.0017277589473254885\n",
      "Epoch 5300==> Loss = 0.00171729812377467\n",
      "Epoch 5400==> Loss = 0.0017072391354053045\n",
      "Epoch 5500==> Loss = 0.00169755970058738\n",
      "Epoch 5600==> Loss = 0.00168823910935434\n",
      "Epoch 5700==> Loss = 0.001679258091724434\n",
      "Epoch 5800==> Loss = 0.0016705986985709191\n",
      "Epoch 5900==> Loss = 0.0016622441937215592\n",
      "Epoch 6000==> Loss = 0.001654178956116437\n",
      "Epoch 6100==> Loss = 0.0016463883909872696\n",
      "Epoch 6200==> Loss = 0.0016388588491347404\n",
      "Epoch 6300==> Loss = 0.0016315775534845322\n",
      "Epoch 6400==> Loss = 0.0016245325321917945\n",
      "Epoch 6500==> Loss = 0.0016177125576438834\n",
      "Epoch 6600==> Loss = 0.0016111070907813883\n",
      "Epoch 6700==> Loss = 0.0016047062302194412\n",
      "Epoch 6800==> Loss = 0.0015985006657069118\n",
      "Epoch 6900==> Loss = 0.0015924816355087303\n",
      "Epoch 7000==> Loss = 0.0015866408873418082\n",
      "Epoch 7100==> Loss = 0.0015809706425308505\n",
      "Epoch 7200==> Loss = 0.0015754635630866242\n",
      "Epoch 7300==> Loss = 0.0015701127214389436\n",
      "Epoch 7400==> Loss = 0.0015649115725826603\n",
      "Epoch 7500==> Loss = 0.001559853928420438\n",
      "Epoch 7600==> Loss = 0.0015549339341070284\n",
      "Epoch 7700==> Loss = 0.0015501460462183946\n",
      "Epoch 7800==> Loss = 0.0015454850125869001\n",
      "Epoch 7900==> Loss = 0.001540945853658818\n",
      "Epoch 8000==> Loss = 0.001536523845243926\n",
      "Epoch 8100==> Loss = 0.0015322145025396983\n",
      "Epoch 8200==> Loss = 0.0015280135653232931\n",
      "Epoch 8300==> Loss = 0.0015239169842143534\n",
      "Epoch 8400==> Loss = 0.0015199209079211492\n",
      "Epoch 8500==> Loss = 0.0015160216713894477\n",
      "Epoch 8600==> Loss = 0.0015122157847823627\n",
      "Epoch 8700==> Loss = 0.0015084999232241496\n",
      "Epoch 8800==> Loss = 0.0015048709172484115\n",
      "Epoch 8900==> Loss = 0.0015013257438947177\n",
      "Epoch 9000==> Loss = 0.0014978615184045537\n",
      "Epoch 9100==> Loss = 0.001494475486469895\n",
      "Epoch 9200==> Loss = 0.0014911650169926068\n",
      "Epoch 9300==> Loss = 0.0014879275953166512\n",
      "Epoch 9400==> Loss = 0.001484760816897142\n",
      "Epoch 9500==> Loss = 0.001481662381374814\n",
      "Epoch 9600==> Loss = 0.0014786300870257632\n",
      "Epoch 9700==> Loss = 0.0014756618255594405\n",
      "Epoch 9800==> Loss = 0.0014727555772398784\n",
      "Epoch 9900==> Loss = 0.0014699094063073234\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "w, b, l = fit(X_train, y_train, lr=0.5, c=len(classes), epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.9515503875969%'"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy for training set.\n",
    "train_preds = predict(X_train, w, b)\n",
    "accuracy(y_train, train_preds)\n",
    "\n",
    "# Accuracy for test set.\n",
    "# Flattening and normalizing.\n",
    "test_preds = predict(X_test, w, b)\n",
    "accuracy(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000000e+00, 1.26853142e-13, 4.08548538e-14, 1.92079102e-14,\n",
       "        1.12305135e-14]),\n",
       " [41, 37, 35, 33, 39],\n",
       " {'Covid': '99.99999999997522%',\n",
       "  'Acne': '1.2685314159533862e-11%',\n",
       "  'Arthritis': '4.0854853761290825e-12%',\n",
       "  'Hypoglycemia': '1.920791016937742e-12%',\n",
       "  'Psoriasis': '1.123051350896803e-12%'})"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[5115]\n",
    "predict_top(x, w, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_x():\n",
    "    n = random.randint(10,30)\n",
    "    random_index = random.sample(range(1, len(X[5] - 1)), n)\n",
    "    zero_matrix = np.zeros(len(X[5]))\n",
    "    for i in random_index:\n",
    "        zero_matrix[i] = 1\n",
    "    return n, zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.14905524, 0.11857408, 0.10086413, 0.08105034, 0.06697267]),\n",
       " [18, 31, 10, 14, 27],\n",
       " {'Typhoid': '14.9055239648949%',\n",
       "  'Hypothyroidism': '11.857408249118642%',\n",
       "  'Hypertension ': '10.086412606554193%',\n",
       "  'Jaundice': '8.105034113143612%',\n",
       "  'Pneumonia': '6.697267069648662%'})"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, k = create_random_x()\n",
    "n, k\n",
    "predict_top(k, w, b, 5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
