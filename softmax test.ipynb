{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n",
      "[0.14409682 0.05301026 0.01950138 0.39169577 0.39169577]\n"
     ]
    }
   ],
   "source": [
    "# softmax regression\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "\n",
    "z = np.array([5,4,3,6,6])\n",
    "\n",
    "def softmax(z):\n",
    "    \n",
    "    # z--> linear part.\n",
    "    \n",
    "    # subtracting the max of z for numerical stability.\n",
    "    exp = np.exp(z - np.max(z))\n",
    "    \n",
    "    # Calculating softmax for all examples.\n",
    "    for i in range(len(z)):\n",
    "        exp[i] /= np.sum(exp[i])\n",
    "        \n",
    "    return exp\n",
    "\n",
    "def softmax_stable(Z):\n",
    "    e_Z = np.exp(Z - np.max(Z, axis = 0, keepdims = True))\n",
    "    A = e_Z / e_Z.sum(axis = 0)\n",
    "    return A\n",
    "\n",
    "def number_classes(y):\n",
    "    c = 0\n",
    "    lst_class = []\n",
    "    dict = {}\n",
    "    for i in range(len(y)):\n",
    "        if y[i] not in lst_class:\n",
    "            c += 1\n",
    "            lst_class.append(y[i])\n",
    "    return lst_class, c\n",
    "\n",
    "def one_hot(y, c):\n",
    "    \n",
    "    # y--> label/ground truth.\n",
    "    # c--> Number of classes.\n",
    "    \n",
    "    # A zero matrix of size (m, c)\n",
    "    y_hot = np.zeros((len(y), c))\n",
    "    \n",
    "    # Putting 1 for column where the label is,\n",
    "    # Using multidimensional indexing.\n",
    "    y_hot[np.arange(len(y)), y] = 1\n",
    "    \n",
    "    return y_hot\n",
    "\n",
    "def predict(X, w, b):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # w --> weights.\n",
    "    # b --> bias.\n",
    "    \n",
    "    # Predicting\n",
    "    z = X@w + b\n",
    "    y_hat = softmax(z)\n",
    "    \n",
    "    # Returning the class with highest probability.\n",
    "    return np.argmax(y_hat, axis=1)\n",
    "\n",
    "def accuracy(y, y_hat):\n",
    "    return str(np.sum(y==y_hat)/len(y)*100) +\"%\"\n",
    "\n",
    "print(softmax(z))\n",
    "print(softmax_stable(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_top(x, w, b, n):\n",
    "    z = x@w + b\n",
    "    y_hot = softmax_stable(z)\n",
    "    \n",
    "    #sort the y_hot\n",
    "    sort = np.sort(y_hot)\n",
    "    sort_convert = sort[::-1]\n",
    "    \n",
    "    top_n = sort_convert[:n]\n",
    "    top_n_index = []\n",
    "    for j in range(len(top_n)):\n",
    "        for i in range(len(y_hot)):\n",
    "            if y_hot[i] == top_n[j]:\n",
    "                top_n_index.append(i)\n",
    "    \n",
    "    top_list = {}\n",
    "    for i in range(n):\n",
    "        top_list[(dict_convert[classes[top_n_index[i]]])] = str(sort_convert[i] * 100) + \"%\"\n",
    "    return top_n, top_n_index, top_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"out.csv\")\n",
    "df_raw = df.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_raw.iloc[:,:-1].to_numpy()\n",
    "y = df_raw.iloc[:,-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of classes\n",
    "\n",
    "dict = {}\n",
    "count = 0\n",
    "for x in y:\n",
    "    if x not in dict:\n",
    "        dict[x] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary of convert classes\n",
    "dict_convert = {}\n",
    "for x in dict:\n",
    "    dict_convert[dict[x]] = x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#change text to integer\n",
    "for i in range(len(y)):\n",
    "    y[i] = dict[y[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, c = number_classes(y)\n",
    "y_hot = one_hot(y, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = -np.mean(np.log(y_hot[np.arange(len(y)), y]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, y, lr, c, epochs):\n",
    "    \n",
    "    # X --> Input.\n",
    "    # y --> true/target value.\n",
    "    # lr --> Learning rate.\n",
    "    # c --> Number of classes.\n",
    "    # epochs --> Number of iterations.\n",
    "    \n",
    "        \n",
    "    # m-> number of training examples\n",
    "    # n-> number of features \n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Initializing weights and bias randomly.\n",
    "    w = np.random.random((n, c))\n",
    "    b = np.random.random(c)\n",
    "    # Empty list to store losses.\n",
    "    losses = []\n",
    "    \n",
    "    # Training loop.\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Calculating hypothesis/prediction.\n",
    "        z = X@w + b\n",
    "        y_hat = softmax(z)\n",
    "        \n",
    "        # One-hot encoding y.\n",
    "        y_hot = one_hot(y, c)\n",
    "        \n",
    "        # Calculating the gradient of loss w.r.t w and b.\n",
    "        w_grad = (1/m)*np.dot(X.T, (y_hat - y_hot)) \n",
    "        b_grad = (1/m)*np.sum(y_hat - y_hot)\n",
    "        \n",
    "        # Updating the parameters.\n",
    "        w = w - lr*w_grad\n",
    "        b = b - lr*b_grad\n",
    "        \n",
    "        # Calculating loss and appending it in the list.\n",
    "        loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n",
    "        losses.append(loss)\n",
    "        # Printing out the loss at every 100th iteration.\n",
    "        if epoch%100==0:\n",
    "            print('Epoch {epoch}==> Loss = {loss}'\n",
    "                  .format(epoch=epoch, loss=loss))\n",
    "    return w, b, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train vs sample data\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0==> Loss = 19.51958168265143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-aeef52f5c68f>:39: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.mean(np.log(y_hat[np.arange(len(y)), y]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 100==> Loss = 0.010982989765766353\n",
      "Epoch 200==> Loss = 0.008157752047960348\n",
      "Epoch 300==> Loss = 0.006954883799946346\n",
      "Epoch 400==> Loss = 0.006220916203022421\n",
      "Epoch 500==> Loss = 0.00570184899307882\n",
      "Epoch 600==> Loss = 0.005303172953060633\n",
      "Epoch 700==> Loss = 0.004980455235638506\n",
      "Epoch 800==> Loss = 0.0047096327329077025\n",
      "Epoch 900==> Loss = 0.004476372673514642\n",
      "Epoch 1000==> Loss = 0.004271538535679611\n",
      "Epoch 1100==> Loss = 0.004089005050687278\n",
      "Epoch 1200==> Loss = 0.003924502436988381\n",
      "Epoch 1300==> Loss = 0.0037749583660765355\n",
      "Epoch 1400==> Loss = 0.0036381007146445883\n",
      "Epoch 1500==> Loss = 0.003512206754388157\n",
      "Epoch 1600==> Loss = 0.003395939664905003\n",
      "Epoch 1700==> Loss = 0.0032882394944502287\n",
      "Epoch 1800==> Loss = 0.003188248808019038\n",
      "Epoch 1900==> Loss = 0.0030952605376239587\n",
      "Epoch 2000==> Loss = 0.0030086802265342025\n",
      "Epoch 2100==> Loss = 0.002927998038185063\n",
      "Epoch 2200==> Loss = 0.002852767870926776\n",
      "Epoch 2300==> Loss = 0.002782591945556029\n",
      "Epoch 2400==> Loss = 0.002717109686112931\n",
      "Epoch 2500==> Loss = 0.002655989920388506\n",
      "Epoch 2600==> Loss = 0.0025989255666989384\n",
      "Epoch 2700==> Loss = 0.0025456301106656512\n",
      "Epoch 2800==> Loss = 0.002495835315115082\n",
      "Epoch 2900==> Loss = 0.002449289737390623\n",
      "Epoch 3000==> Loss = 0.0024057577426209098\n",
      "Epoch 3100==> Loss = 0.002365018795107717\n",
      "Epoch 3200==> Loss = 0.002326866883069079\n",
      "Epoch 3300==> Loss = 0.002291109986634\n",
      "Epoch 3400==> Loss = 0.002257569538201627\n",
      "Epoch 3500==> Loss = 0.0022260798511489687\n",
      "Epoch 3600==> Loss = 0.002196487510282456\n",
      "Epoch 3700==> Loss = 0.00216865072787193\n",
      "Epoch 3800==> Loss = 0.0021424386746313317\n",
      "Epoch 3900==> Loss = 0.0021177307972296716\n",
      "Epoch 4000==> Loss = 0.0020944161340434066\n",
      "Epoch 4100==> Loss = 0.002072392639771233\n",
      "Epoch 4200==> Loss = 0.0020515665278375795\n",
      "Epoch 4300==> Loss = 0.0020318516376070963\n",
      "Epoch 4400==> Loss = 0.002013168831567733\n",
      "Epoch 4500==> Loss = 0.001995445425951937\n",
      "Epoch 4600==> Loss = 0.0019786146568207623\n",
      "Epoch 4700==> Loss = 0.001962615182450567\n",
      "Epoch 4800==> Loss = 0.0019473906219296038\n",
      "Epoch 4900==> Loss = 0.0019328891291635684\n",
      "Epoch 5000==> Loss = 0.00191906300097698\n",
      "Epoch 5100==> Loss = 0.0019058683176456025\n",
      "Epoch 5200==> Loss = 0.001893264613973042\n",
      "Epoch 5300==> Loss = 0.0018812145789063497\n",
      "Epoch 5400==> Loss = 0.0018696837816413697\n",
      "Epoch 5500==> Loss = 0.0018586404221861935\n",
      "Epoch 5600==> Loss = 0.001848055104407038\n",
      "Epoch 5700==> Loss = 0.0018379006296664808\n",
      "Epoch 5800==> Loss = 0.0018281518092679455\n",
      "Epoch 5900==> Loss = 0.0018187852940337056\n",
      "Epoch 6000==> Loss = 0.0018097794194634862\n",
      "Epoch 6100==> Loss = 0.0018011140650394221\n",
      "Epoch 6200==> Loss = 0.0017927705263604298\n",
      "Epoch 6300==> Loss = 0.0017847313989008\n",
      "Epoch 6400==> Loss = 0.0017769804722949525\n",
      "Epoch 6500==> Loss = 0.0017695026341490274\n",
      "Epoch 6600==> Loss = 0.0017622837824741912\n",
      "Epoch 6700==> Loss = 0.00175531074591968\n",
      "Epoch 6800==> Loss = 0.001748571211064723\n",
      "Epoch 6900==> Loss = 0.0017420536560979819\n",
      "Epoch 7000==> Loss = 0.0017357472902795605\n",
      "Epoch 7100==> Loss = 0.0017296419986394676\n",
      "Epoch 7200==> Loss = 0.00172372829142037\n",
      "Epoch 7300==> Loss = 0.0017179972578201081\n",
      "Epoch 7400==> Loss = 0.0017124405236340686\n",
      "Epoch 7500==> Loss = 0.0017070502124362959\n",
      "Epoch 7600==> Loss = 0.0017018189099741712\n",
      "Epoch 7700==> Loss = 0.0016967396314826499\n",
      "Epoch 7800==> Loss = 0.001691805791653657\n",
      "Epoch 7900==> Loss = 0.001687011177021264\n",
      "Epoch 8000==> Loss = 0.0016823499205463712\n",
      "Epoch 8100==> Loss = 0.001677816478207158\n",
      "Epoch 8200==> Loss = 0.001673405607416995\n",
      "Epoch 8300==> Loss = 0.0016691123471118754\n",
      "Epoch 8400==> Loss = 0.0016649319993622303\n",
      "Epoch 8500==> Loss = 0.0016608601123785508\n",
      "Epoch 8600==> Loss = 0.0016568924647919262\n",
      "Epoch 8700==> Loss = 0.0016530250511024493\n",
      "Epoch 8800==> Loss = 0.001649254068197697\n",
      "Epoch 8900==> Loss = 0.0016455759028525508\n",
      "Epoch 9000==> Loss = 0.0016419871201296516\n",
      "Epoch 9100==> Loss = 0.0016384844526076785\n",
      "Epoch 9200==> Loss = 0.001635064790370223\n",
      "Epoch 9300==> Loss = 0.0016317251716947226\n",
      "Epoch 9400==> Loss = 0.0016284627743857745\n",
      "Epoch 9500==> Loss = 0.0016252749077026157\n",
      "Epoch 9600==> Loss = 0.0016221590048342244\n",
      "Epoch 9700==> Loss = 0.001619112615879996\n",
      "Epoch 9800==> Loss = 0.0016161334012975812\n",
      "Epoch 9900==> Loss = 0.001613219125781876\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "w, b, l = fit(X_train, y_train, lr=0.5, c=len(classes), epochs=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'99.9515503875969%'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy for training set.\n",
    "train_preds = predict(X_train, w, b)\n",
    "accuracy(y_train, train_preds)\n",
    "\n",
    "# Accuracy for test set.\n",
    "# Flattening and normalizing.\n",
    "test_preds = predict(X_test, w, b)\n",
    "accuracy(y_test, test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.00000000e+00, 1.28953552e-17, 4.90982970e-19, 4.85313488e-19,\n",
       "        3.56477959e-19]),\n",
       " [41, 35, 4, 0, 10],\n",
       " {'Covid': '100.0%',\n",
       "  'Arthritis': '1.2895355175031855e-15%',\n",
       "  'Drug Reaction': '4.909829702074874e-17%',\n",
       "  'Fungal infection': '4.8531348784232774e-17%',\n",
       "  'Hypertension ': '3.564779591314934e-17%'})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = X[5115]\n",
    "predict_top(x, w, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_x():\n",
    "    n = random.randint(10,30)\n",
    "    random_index = random.sample(range(1, len(X[5] - 1)), n)\n",
    "    zero_matrix = np.zeros(len(X[5]))\n",
    "    for i in random_index:\n",
    "        zero_matrix[i] = 1\n",
    "    return n, zero_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.26402925, 0.12136285, 0.1189559 , 0.11545562, 0.10302569]),\n",
       " [31, 25, 13, 4, 41],\n",
       " {'Hypothyroidism': '26.40292494295153%',\n",
       "  'Tuberculosis': '12.136285338762296%',\n",
       "  'Paralysis (brain hemorrhage)': '11.89558971231476%',\n",
       "  'Drug Reaction': '11.545562201626975%',\n",
       "  'Covid': '10.302569435421411%'})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n, k = create_random_x()\n",
    "n, k\n",
    "predict_top(k, w, b, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.00 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\data\\anacoda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#train model with sklearn\n",
    "import numpy as np \n",
    "from mnist import MNIST\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# train\n",
    "logreg = linear_model.LogisticRegression(C=1e5, solver = 'lbfgs', multi_class = 'multinomial')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# test\n",
    "y_pred = logreg.predict(X_test)\n",
    "print (\"Accuracy: %.2f %%\" %(100*accuracy_score(y_test, y_pred.tolist())))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
